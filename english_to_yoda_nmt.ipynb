{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "english-to-yoda-nmt",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO13j/TxfKiA+g5NSdzawD3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aravind-sundaresan/yoda-english-nmt/blob/master/english_to_yoda_nmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wLh-JLHte4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST_KwDrN3PIw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0d4ecd5b-2d2f-470a-971f-5bffb9ecee27"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKQSMCyh42uU",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "**Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i0tzJJ_50Xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating 2 dictionaries each (word-index and index-word) for the source and target languages\n",
        "\n",
        "with open('/drive/My Drive/data/english_vocabulary.txt', 'r') as f:\n",
        "  words = f.readlines()\n",
        "\n",
        "english_index_to_word_dict = dict([(index, word.rstrip('\\n')) for index, word in enumerate(words)])\n",
        "english_word_to_index_dict = dict([(word.rstrip('\\n'), index) for index, word in enumerate(words)])\n",
        "\n",
        "with open('/drive/My Drive/data/yoda_vocabulary.txt', 'r') as f:\n",
        "  words = f.readlines()\n",
        "\n",
        "yoda_index_to_word_dict = dict([(index, word.rstrip('\\n')) for index, word in enumerate(words)])\n",
        "yoda_word_to_index_dict = dict([(word.rstrip('\\n'), index) for index, word in enumerate(words)])\n",
        "\n",
        "# Obtaining the number of unique tokens in each vocabulary\n",
        "english_vocab_length = len(english_word_to_index_dict)\n",
        "yoda_vocab_length = len(yoda_word_to_index_dict)\n",
        "\n",
        "'''\n",
        "Limiting the lengths of the sequences (in terms of number of words) in both the source and target languages \n",
        "For source language, max. length = 15 (97% of the sentences have length <= 15)\n",
        "For target language, max. length = 20 (97% of the sentence have length <= 20)\n",
        "'''\n",
        "max_length_source, max_length_target = 15, 20 \n",
        "\n",
        "english_sentences, yoda_english_sentences = [], []\n",
        "with open('/drive/My Drive/data/english_sentences.txt', 'r') as fp: \n",
        "  line = fp.readline()\n",
        "  while line:\n",
        "    line = line.rstrip(\"\\n\")\n",
        "    english_sentences.append([int(token) for token in line.split(\" \")])\n",
        "    line = fp.readline()\n",
        "\n",
        "with open('/drive/My Drive/data/yoda_english_sentences.txt', 'r') as fp:\n",
        "  line = fp.readline()\n",
        "  while line:\n",
        "    # Adding the start and end tokens to the target sentences\n",
        "    line = \"1 \" + line.rstrip(\"\\n\") + \" 2\"\n",
        "    yoda_english_sentences.append([int(token) for token in line.split(\" \")])\n",
        "    line = fp.readline() \n",
        "\n",
        "\n",
        "# Padding the source and target sentences to ensure that all of them have the same length\n",
        "encoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(english_sentences, maxlen=max_length_source, padding='post')\n",
        "decoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(yoda_english_sentences, maxlen=max_length_target, padding='post')\n",
        "\n",
        "# Splitting the data into training, validation/dev and test sets\n",
        "encoder_input_train, encoder_input_test, decoder_input_train, decoder_input_test = train_test_split(encoder_input_data[:115200], decoder_input_data[:115200], test_size=0.1)\n",
        "encoder_input_train, encoder_input_val, decoder_input_train, decoder_input_val = train_test_split(encoder_input_train, decoder_input_train, test_size=0.1)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqKw9ZMGEgml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(X, y, batch_size=256):\n",
        "    # Function to generate a batch of training examples using a generator object because the dataset is too large to fit into memory\n",
        "    while True:\n",
        "        for i in range(0, len(X), batch_size):            \n",
        "            encoder_input_data = X[i:i+batch_size]\n",
        "            decoder_input_data = y[i:i+batch_size]\n",
        "            \n",
        "            '''\n",
        "             The output of the decoder uses the one-hot representation of each word in a sentence because the output of the seq2seq model\n",
        "             is obtained from a softmax unit. \n",
        "             The size of the decoder output sentence would be (max_length_target, yoda_vocab_length).\n",
        "             The decoder output does not start with the \"_GO\" token. The rest of the content is the same as that of decoder input. \n",
        "             So the decoder output can be defined as the decoder input shifted or offset by one timestep.\n",
        "            '''\n",
        "            decoder_output_data = np.zeros((batch_size, max_length_target, yoda_vocab_length), dtype='float32')\n",
        "\n",
        "            for j in range(len(decoder_input_data)):\n",
        "                for k in range(1, max_length_target):\n",
        "                    decoder_output_data[j, k-1, decoder_input_data[j, k]] = 1\n",
        "            \n",
        "            yield([encoder_input_data, decoder_input_data], decoder_output_data)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4rKlQCwB3r2",
        "colab_type": "text"
      },
      "source": [
        "**Building the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2e7oIj-uKbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Size of word embeddings\n",
        "embedding_dimensions = 50\n",
        "\n",
        "encoder_input = Input(shape=(None,))\n",
        "\n",
        "# Embeddings of the English words fed as input to the encoder network\n",
        "encoder_embeddings = Embedding(english_vocab_length, embedding_dimensions)(encoder_input)\n",
        "\n",
        "# Encoder LSTM layer\n",
        "encoder_lstm = LSTM(embedding_dimensions, return_state=True)\n",
        "encoder_output, encoder_hidden_state, encoder_cell_state = encoder_lstm(encoder_embeddings)\n",
        "\n",
        "# The encoder outputs are discarded and only the hidden and cell states of the encoder are retained\n",
        "encoder_states = [encoder_hidden_state, encoder_cell_state]\n",
        "\n",
        "# Setting up the decoder. The initial state of the decoder is obtained from the encoder_states.\n",
        "\n",
        "decoder_input = Input(shape=(None,))\n",
        "\n",
        "# Embeddings of the Yoda English words fed as input to the decoder network\n",
        "decoder_embeddings = Embedding(yoda_vocab_length, embedding_dimensions)(decoder_input)\n",
        "\n",
        "# Decoder LSTM layer\n",
        "decoder_lstm = LSTM(embedding_dimensions, return_sequences=True, return_state=True)\n",
        "\n",
        "decoder_output, _, _ = decoder_lstm(decoder_embeddings, initial_state=encoder_states)\n",
        "\n",
        "# Using a softmax unit to generate a probability distribution over the target vocabulary for each time step\n",
        "decoder_dense = Dense(yoda_vocab_length, activation='softmax')\n",
        "decoder_output = decoder_dense(decoder_output)\n",
        "\n",
        "# Defining the model\n",
        "model = tf.keras.Model([encoder_input, decoder_input], decoder_output)\n",
        "# Compiling the model\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeKv9xR0CEi3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "7c5f4b17-8731-4dbc-94df-04dce2749685"
      },
      "source": [
        "# Training the model\n",
        "training_set_size = len(encoder_input_train)\n",
        "validation_set_size = len(encoder_input_val)\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "model.fit_generator(generator=generate_batch(encoder_input_train, decoder_input_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=training_set_size//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=generate_batch(encoder_input_val, decoder_input_val, batch_size=batch_size),\n",
        "                    validation_steps=validation_set_size//batch_size)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-e17f565739da>:11: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/10\n",
            "729/729 [==============================] - 724s 994ms/step - loss: 3.8044 - acc: 0.4933 - val_loss: 2.9474 - val_acc: 0.6052\n",
            "Epoch 2/10\n",
            "729/729 [==============================] - 721s 989ms/step - loss: 2.7588 - acc: 0.6297 - val_loss: 2.6671 - val_acc: 0.6447\n",
            "Epoch 3/10\n",
            "729/729 [==============================] - 720s 987ms/step - loss: 2.5664 - acc: 0.6548 - val_loss: 2.5722 - val_acc: 0.6539\n",
            "Epoch 4/10\n",
            "729/729 [==============================] - 726s 996ms/step - loss: 2.4464 - acc: 0.6695 - val_loss: 2.4469 - val_acc: 0.6720\n",
            "Epoch 5/10\n",
            "729/729 [==============================] - 731s 1s/step - loss: 2.3510 - acc: 0.6816 - val_loss: 2.3597 - val_acc: 0.6827\n",
            "Epoch 6/10\n",
            "729/729 [==============================] - 730s 1s/step - loss: 2.2611 - acc: 0.6928 - val_loss: 2.2882 - val_acc: 0.6929\n",
            "Epoch 7/10\n",
            "729/729 [==============================] - 734s 1s/step - loss: 2.1825 - acc: 0.7028 - val_loss: 2.2257 - val_acc: 0.7012\n",
            "Epoch 8/10\n",
            "729/729 [==============================] - 748s 1s/step - loss: 2.1153 - acc: 0.7112 - val_loss: 2.1781 - val_acc: 0.7070\n",
            "Epoch 9/10\n",
            "729/729 [==============================] - 748s 1s/step - loss: 2.0503 - acc: 0.7185 - val_loss: 2.1232 - val_acc: 0.7127\n",
            "Epoch 10/10\n",
            "729/729 [==============================] - 744s 1s/step - loss: 1.9913 - acc: 0.7252 - val_loss: 2.0765 - val_acc: 0.7188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff484551780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXR4HuT1Oimt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the weights of the trained model\n",
        "model.save_weights('/drive/My Drive/data/yoda_nmt_weights.h5')\n",
        "\n",
        "model.load_weights('/drive/My Drive/data/yoda_nmt_weights.h5')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbzjfAE_pZay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model setup for Inference\n",
        "\n",
        "# Defining the encoder model to obtain the (hidden and cell) states representing the context vector\n",
        "encoder = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Defining the decoder model for inference\n",
        "\n",
        "# Hidden and cells states of the decoder from the previous time step\n",
        "decoder_cell_state_input = Input(shape=(embedding_dimensions,))\n",
        "decoder_hidden_state_input = Input(shape=(embedding_dimensions,))\n",
        "decoder_states_input = [decoder_hidden_state_input, decoder_cell_state_input]\n",
        "\n",
        "decoder_embeddings_inference = Embedding(yoda_vocab_length, embedding_dimensions)(decoder_input)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_inference_output, decoder_hidden_state_output, decoder_cell_state_output = decoder_lstm(decoder_embeddings_inference, initial_state=decoder_states_input)\n",
        "decoder_states_output = [decoder_hidden_state_output, decoder_cell_state_output]\n",
        "decoder_inference_output = decoder_dense(decoder_inference_output)\n",
        "\n",
        "decoder = Model([decoder_input] + decoder_states_input, [decoder_inference_output] + decoder_states_output)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sKGoGXRQvvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Translation Function\n",
        "\n",
        "def translate(input_sequence):\n",
        "    \n",
        "    # Obtain the states of the encoder after processing the input sequence\n",
        "    model_states = encoder.predict(input_sequence)\n",
        "    \n",
        "    # Initializing the target sequence to store the output and setting the first character as the \"Start\" token\n",
        "    target_sequence = np.zeros((1, 1))\n",
        "    target_sequence[0, 0] = yoda_word_to_index_dict['_GO']\n",
        "    \n",
        "    stop_condition = False\n",
        "    output_sentence = ''\n",
        "    \n",
        "    while stop_condition is not True:\n",
        "        output_tokens, hidden_state, cell_state = decoder.predict([target_sequence] + model_states)\n",
        "    \n",
        "        # Sample a token/word with the highest probability from the output of the sigmoid unit\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = yoda_index_to_word_dict[sampled_token_index]\n",
        "        output_sentence += sampled_token + ' '\n",
        "        \n",
        "        # Checking if the translation is done\n",
        "        if sampled_token == '_EOS' or len(output_sentence) > 50:\n",
        "            stop_condition = True\n",
        "        \n",
        "        # Updating the target sequence\n",
        "        target_sequence = np.zeros((1, 1))\n",
        "        target_sequence[0, 0] = sampled_token\n",
        "        \n",
        "        # Updating the states\n",
        "        model_states = [hidden_state, cell_state]\n",
        "    \n",
        "    return output_sentence"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hkg35QILYu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}