{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "english-to-yoda-nmt",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNoW2W2pxnBJMxC4oU55n03",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aravind-sundaresan/yoda-english-nmt/blob/master/english_to_yoda_nmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wLh-JLHte4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST_KwDrN3PIw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9d52197c-a861-4d65-ed0d-59b135539f94"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKQSMCyh42uU",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "**Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i0tzJJ_50Xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating 2 dictionaries each (word-index and index-word) for the source and target languages\n",
        "\n",
        "with open('/drive/My Drive/data/english_vocabulary.txt', 'r') as f:\n",
        "  words = f.readlines()\n",
        "\n",
        "english_index_to_word_dict = dict([(index, word.rstrip('\\n')) for index, word in enumerate(words)])\n",
        "english_word_to_index_dict = dict([(word.rstrip('\\n'), index) for index, word in enumerate(words)])\n",
        "\n",
        "with open('/drive/My Drive/data/yoda_vocabulary.txt', 'r') as f:\n",
        "  words = f.readlines()\n",
        "\n",
        "yoda_index_to_word_dict = dict([(index, word.rstrip('\\n')) for index, word in enumerate(words)])\n",
        "yoda_word_to_index_dict = dict([(word.rstrip('\\n'), index) for index, word in enumerate(words)])\n",
        "\n",
        "# Obtaining the number of unique tokens in each vocabulary\n",
        "english_vocab_length = len(english_word_to_index_dict)\n",
        "yoda_vocab_length = len(yoda_word_to_index_dict)\n",
        "\n",
        "'''\n",
        "Limiting the lengths of the sequences (in terms of number of words) in both the source and target languages \n",
        "For source language, max. length = 15 (97% of the sentences have length <= 15)\n",
        "For target language, max. length = 20 (97% of the sentence have length <= 20)\n",
        "'''\n",
        "max_length_source, max_length_target = 15, 20 \n",
        "\n",
        "english_sentences, yoda_english_sentences = [], []\n",
        "with open('/drive/My Drive/data/english_sentences.txt', 'r') as fp: \n",
        "  line = fp.readline()\n",
        "  while line:\n",
        "    line = line.rstrip(\"\\n\")\n",
        "    english_sentences.append([int(token) for token in line.split(\" \")])\n",
        "    line = fp.readline()\n",
        "\n",
        "with open('/drive/My Drive/data/yoda_english_sentences.txt', 'r') as fp:\n",
        "  line = fp.readline()\n",
        "  while line:\n",
        "    # Adding the start and end tokens to the target sentences\n",
        "    line = \"1 \" + line.rstrip(\"\\n\") + \" 2\"\n",
        "    yoda_english_sentences.append([int(token) for token in line.split(\" \")])\n",
        "    line = fp.readline() \n",
        "\n",
        "\n",
        "# Padding the source and target sentences to ensure that all of them have the same length\n",
        "encoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(english_sentences, maxlen=max_length_source, padding='post')\n",
        "decoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(yoda_english_sentences, maxlen=max_length_target, padding='post')\n",
        "\n",
        "# Splitting the data into training, validation/dev and test sets\n",
        "encoder_input_train, encoder_input_test, decoder_input_train, decoder_input_test = train_test_split(encoder_input_data[:115200], decoder_input_data[:115200], test_size=0.1)\n",
        "encoder_input_train, encoder_input_val, decoder_input_train, decoder_input_val = train_test_split(encoder_input_train, decoder_input_train, test_size=0.1)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqKw9ZMGEgml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(X, y, batch_size=256):\n",
        "    # Function to generate a batch of training examples using a generator object because the dataset is too large to fit into memory\n",
        "    while True:\n",
        "        for i in range(0, len(X), batch_size):            \n",
        "            encoder_input_data = X[i:i+batch_size]\n",
        "            decoder_input_data = y[i:i+batch_size]\n",
        "            \n",
        "            '''\n",
        "             The output of the decoder uses the one-hot representation of each word in a sentence because the output of the seq2seq model\n",
        "             is obtained from a softmax unit. \n",
        "             The size of the decoder output sentence would be (max_length_target, yoda_vocab_length).\n",
        "             The decoder output does not start with the \"_GO\" token. The rest of the content is the same as that of decoder input. \n",
        "             So the decoder output can be defined as the decoder input shifted or offset by one timestep.\n",
        "            '''\n",
        "            decoder_output_data = np.zeros((batch_size, max_length_target, yoda_vocab_length), dtype='float32')\n",
        "\n",
        "            for j in range(len(decoder_input_data)):\n",
        "                for k in range(1, max_length_target):\n",
        "                    decoder_output_data[j, k-1, decoder_input_data[j, k]] = 1\n",
        "            \n",
        "            yield([encoder_input_data, decoder_input_data], decoder_output_data)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4rKlQCwB3r2",
        "colab_type": "text"
      },
      "source": [
        "**Building the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2e7oIj-uKbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Size of word embeddings\n",
        "embedding_dimensions = 50\n",
        "\n",
        "encoder_input = Input(shape=(None,))\n",
        "\n",
        "# Embeddings of the English words fed as input to the encoder network\n",
        "encoder_embeddings = Embedding(english_vocab_length, embedding_dimensions)(encoder_input)\n",
        "\n",
        "# Encoder LSTM layer\n",
        "encoder_lstm = LSTM(embedding_dimensions, return_state=True)\n",
        "encoder_output, encoder_hidden_state, encoder_cell_state = encoder_lstm(encoder_embeddings)\n",
        "\n",
        "# The encoder outputs are discarded and only the hidden and cell states of the encoder are retained\n",
        "encoder_states = [encoder_hidden_state, encoder_cell_state]\n",
        "\n",
        "# Setting up the decoder. The initial state of the decoder is obtained from the encoder_states.\n",
        "\n",
        "decoder_input = Input(shape=(None,))\n",
        "\n",
        "# Embeddings of the Yoda English words fed as input to the decoder network\n",
        "decoder_embeddings = Embedding(yoda_vocab_length, embedding_dimensions)(decoder_input)\n",
        "\n",
        "# Decoder LSTM layer\n",
        "decoder_lstm = LSTM(embedding_dimensions, return_sequences=True, return_state=True)\n",
        "\n",
        "decoder_output, _, _ = decoder_lstm(decoder_embeddings, initial_state=encoder_states)\n",
        "\n",
        "# Using a softmax unit to generate a probability distribution over the target vocabulary for each time step\n",
        "decoder_dense = Dense(yoda_vocab_length, activation='softmax')\n",
        "decoder_output = decoder_dense(decoder_output)\n",
        "\n",
        "# Defining the model\n",
        "model = tf.keras.Model([encoder_input, decoder_input], decoder_output)\n",
        "# Compiling the model\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeKv9xR0CEi3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a946e82e-56a5-49d6-c267-b7e4aaba1508"
      },
      "source": [
        "# Training the model\n",
        "training_set_size = len(encoder_input_train)\n",
        "validation_set_size = len(encoder_input_val)\n",
        "batch_size = 128\n",
        "epochs = 1\n",
        "\n",
        "model.fit_generator(generator=generate_batch(encoder_input_train, decoder_input_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=training_set_size//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=generate_batch(encoder_input_val, decoder_input_val, batch_size=batch_size),\n",
        "                    validation_steps=validation_set_size//batch_size)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-bc9a48d6f3bb>:11: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "729/729 [==============================] - 758s 1s/step - loss: 3.7990 - acc: 0.4874 - val_loss: 2.9447 - val_acc: 0.6082\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f711867e8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXR4HuT1Oimt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the weights of the trained model\n",
        "model.save_weights('yoda_nmt_weights.h5')\n",
        "\n",
        "model.load_weights('yoda_nmt_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbzjfAE_pZay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "da6051a3-f938-4862-9498-014ac46011ae"
      },
      "source": [
        "# Model setup for Inference\n",
        "\n",
        "# Encoder model to encode input sequence into context vectors\n",
        "encoder_model = tf.keras.Model(encoder_input, encoder_states)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 50)          4907950   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  [(None, 50), (None, 50),  20200     \n",
            "=================================================================\n",
            "Total params: 4,928,150\n",
            "Trainable params: 4,928,150\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngVyHVFEL8cv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "93d7956e-fa22-4edb-aab1-3d0608412890"
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 9.0 GB  | Proc size: 11.1 GB\n",
            "GPU RAM Free: 6150MB | Used: 8929MB | Util  59% | Total 15079MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sKGoGXRQvvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}